{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b548c558-7370-4022-a07c-9280f8f26d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Predicción de Captura de Derivaciones Médicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b4400b1-525c-487f-ac85-b140bec3ea94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Contexto del Problema\n",
    "\n",
    "Este proyecto busca predecir la probabilidad de que un paciente capture una derivación médica (es decir, que agende y realice una prestación) dentro del sistema de salud.\n",
    "\n",
    "El modelo tiene como objetivo anticipar el comportamiento del paciente para activar acciones comerciales que aumenten la tasa de captura sin impactar negativamente los ingresos, aplicando descuentos de copago según el riesgo de no captura.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb5b66c-4dc2-4dbd-bc91-320052830580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dataset y definición de variables\n",
    "\n",
    "El dataset incluye información de pacientes derivados a prestaciones, con variables demográficas, clínicas y comportamentales.\n",
    "\n",
    "Se construyeron variables numéricas y categóricas, y se utilizaron técnicas de ingeniería de variables como:\n",
    "- Indexación de variables categóricas\n",
    "- Escalamiento de variables numéricas\n",
    "- Ensamblaje de features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72a61e18-ae32-4c95-90d5-7b4eba4c4a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Modelo de Machine Learning\n",
    "\n",
    "Se utilizó un modelo Gradient Boosted Trees (`GBTClassifier`) con ajuste de hiperparámetros vía `CrossValidator`.\n",
    "\n",
    "Para abordar el desbalance entre pacientes capturados y no capturados, se incluyó una columna de pesos (`weightCol`) que penaliza más los errores en pacientes que sí capturaron.\n",
    "\n",
    "Se evaluó el modelo con métricas:\n",
    "- Precisión general\n",
    "- Área bajo curva ROC\n",
    "- Área bajo curva de precisión (PR)\n",
    "- F1 Score, Precisión y Recall (con ajuste de umbral)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69c1769a-e605-4fea-9387-253dc09f095c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Guardado del modelo y MLflow\n",
    "\n",
    "Se registró el modelo con MLflow y se guardó en Databricks para futuras inferencias. También se registraron:\n",
    "- Hiperparámetros\n",
    "- Métricas de evaluación\n",
    "- Importancia de variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9213263f-d236-475b-8da8-c1f9b19d0f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ajuste de umbral y normalización de probabilidad\n",
    "\n",
    "Se identificó que el mejor umbral para balancear precisión y recall es 0.80.\n",
    "\n",
    "A partir de este umbral, se normalizó la probabilidad para llevarla a una escala 0–1, facilitando la interpretación comercial.\n",
    "\n",
    "Esta probabilidad normalizada se usará en un segundo notebook para aplicar reglas de negocio (como copago 0% o 50%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fff40283-789b-48e4-ae86-88b951c806cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Extracción de Datos\n",
    "Descripción de la fuente, filtros aplicados por fecha, y definición del set de entrenamiento y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "014c21a4-18f8-4cb7-b6ba-6fa21e463dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# \uD83D\uDCE6 Librerías Generales\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "import pickle\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# ================================\n",
    "# \uD83D\uDCC5 Manejo de Fechas\n",
    "# ================================\n",
    "from datetime import date, datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import dateutil.relativedelta\n",
    "\n",
    "# ================================\n",
    "# \uD83D\uDCCA Librerías de Modelado (sklearn)\n",
    "# ================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ================================\n",
    "# ⚙️ Configuración de visualización en Pandas\n",
    "# ================================\n",
    "\n",
    "# Mostrar los números decimales con 2 cifras\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# Mostrar todas las columnas sin truncar en la salida\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4bff544d-add4-49bd-adc6-a518f9f8d254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Función para calcular la edad a partir de la fecha de nacimiento\n",
    "def calcular_edad(df, columna_fecha_nacimiento):\n",
    "    # Convertir la columna de fecha de nacimiento a tipo datetime si no lo está\n",
    "    df[columna_fecha_nacimiento] = pd.to_datetime(df[columna_fecha_nacimiento])\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    hoy = pd.to_datetime('today')\n",
    "\n",
    "    # Calcular la edad fila por fila\n",
    "    df['edad'] = df[columna_fecha_nacimiento].apply(\n",
    "        lambda fecha: hoy.year - fecha.year - ((hoy.month, hoy.day) < (fecha.month, fecha.day))\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Función para obtener la fecha actual con un ajuste de días\n",
    "def FechaActual(d):\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    zona_horaria = pytz.timezone(\"America/Santiago\")  # Cambiar por la zona horaria que necesites  \n",
    "    return (datetime.now(zona_horaria) + timedelta(days=d))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96f52a8f-b35b-46d9-8ef7-7c29da3805c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83D\uDDC2️ BD: Derivación y Captura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9d9063a7-42a1-4840-8b25-66129d5a9c6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Definir variable fecha proceso\n",
    "Fecha_presente = FechaActual(0) \n",
    "Fecha_presente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "47af188a-16a7-414d-b2bb-70e47f9d4ceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- SIMULACIÓN PARA PORTAFOLIO: reemplazo de extracción SQL ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1) Fechas\n",
    "Fecha_presente = pd.Timestamp.today().normalize()\n",
    "ffin = (Fecha_presente - timedelta(days=0)).strftime(\"%Y-%m-%d\")      # hoy\n",
    "finicio = (Fecha_presente - timedelta(days=91)).strftime(\"%Y-%m-%d\")  # 91 días atrás\n",
    "\n",
    "# 2) Generador de datos sintéticos\n",
    "def simular_derivacion_captura(\n",
    "    inicio:str,\n",
    "    fin:str,\n",
    "    seed:int=42,\n",
    "    media_registros_dia:int=120,  # controla volumen\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    fechas = pd.date_range(start=inicio, end=fin, freq=\"D\")\n",
    "\n",
    "    # Catálogos\n",
    "    centros = [\"CM Norte\", \"CM Sur\", \"CM Este\", \"CM Oeste\", \"Clínica Central\"]\n",
    "    zonas = [\"Zona Centro\", \"Zona Norte\", \"Zona Sur\", \"Zona Oriente\", \"Zona Poniente\"]\n",
    "    especialidades = [\"Resonancia\", \"Scanner\"]  # <- según tu condición\n",
    "    linea_negocio = [\"Imagenología\"]\n",
    "    B8_cat = [\n",
    "        \"Neuro\", \"Abdomen\", \"Tórax\", \"Columna\", \"Músculo-esquelético\",\n",
    "        \"Vascular\", \"Pélvico\", \"Otros\"\n",
    "    ]\n",
    "    aseguradora_denom = [\"Seguro_Publico\"]  \n",
    "    prestaciones_res = [\"RM Cráneo\", \"RM Columna\", \"RM Abdomen\", \"RM Rodilla\"]\n",
    "    prestaciones_scn = [\"TC Tórax\", \"TC Abdomen\", \"TC Pelvis\", \"TC Cerebro\"]\n",
    "    usuarios_ag = [\"BOT_WA\", \"CALL_CENTER\", \"PORTAL\", \"PRESENCIAL\"]\n",
    "\n",
    "    # Función para generar un RUT\n",
    "    def rut_fake():\n",
    "        body = rng.integers(5_000_000, 25_000_000)\n",
    "        dv = \"K\" if rng.random() < 0.05 else str(rng.integers(0,10))\n",
    "        return f\"{body}-{dv}\"\n",
    "\n",
    "    registros = []\n",
    "    episodio_correlativo = 10_000_000\n",
    "\n",
    "    for f in fechas:\n",
    "        # Poisson para variabilidad diaria\n",
    "        n = max(0, rng.poisson(media_registros_dia))\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        # Mezclar especialidad para asignar prestaciones coherentes\n",
    "        espec = rng.choice(especialidades, size=n, replace=True)\n",
    "        prest = []\n",
    "        for e in espec:\n",
    "            if e == \"Resonancia\":\n",
    "                prest.append(rng.choice(prestaciones_res))\n",
    "            else:\n",
    "                prest.append(rng.choice(prestaciones_scn))\n",
    "\n",
    "        # Probabilidad de captura y de tener citación\n",
    "        captura_flag = rng.binomial(1, 0.38, size=n)  # ~38% capturan\n",
    "        tiene_cita = rng.binomial(1, 0.55, size=n)    # ~55% tienen cita (indep.)\n",
    "\n",
    "        # Para cada registro del día\n",
    "        for i in range(n):\n",
    "            episodio_correlativo += 1\n",
    "            precio_base = rng.lognormal(mean=np.log(180000), sigma=0.35)  # CLP aprox\n",
    "            precio = float(np.clip(precio_base, 60_000, 600_000))\n",
    "\n",
    "            # Si captura, definimos fecha_cap y centro_cap razonables\n",
    "            if captura_flag[i] == 1:\n",
    "                delta_cap = int(rng.integers(0, 15))  # 0–14 días después\n",
    "                fecha_cap = f + pd.Timedelta(days=delta_cap)\n",
    "                centro_cap = rng.choice(centros)\n",
    "                episodio_cap = episodio_correlativo  # podría coincidir\n",
    "                valor_liquidado = float(np.round(precio * rng.uniform(0.85, 1.1), 0))\n",
    "            else:\n",
    "                fecha_cap = pd.NaT\n",
    "                centro_cap = np.nan\n",
    "                episodio_cap = np.nan\n",
    "                valor_liquidado = np.nan\n",
    "\n",
    "            # Citación PA (puede existir aunque no capture)\n",
    "            if tiene_cita[i] == 1:\n",
    "                # citas entre f-2 y f+30 (simula agendamientos previos o futuros)\n",
    "                delta_cita = int(rng.integers(-2, 31))\n",
    "                fecha_cita_pa = f + pd.Timedelta(days=delta_cita)\n",
    "                fecha_creacion_pa = fecha_cita_pa - pd.Timedelta(days=int(rng.integers(0,7)))\n",
    "                # ~10% anulaciones\n",
    "                if rng.random() < 0.10:\n",
    "                    fecha_anulacion_pa = fecha_cita_pa - pd.Timedelta(days=int(rng.integers(0,3)))\n",
    "                else:\n",
    "                    fecha_anulacion_pa = pd.NaT\n",
    "                id_citacion_pa = int(rng.integers(1_000_000, 9_999_999))\n",
    "                usuario_ag = rng.choice(usuarios_ag)\n",
    "            else:\n",
    "                fecha_cita_pa = pd.NaT\n",
    "                fecha_creacion_pa = pd.NaT\n",
    "                fecha_anulacion_pa = pd.NaT\n",
    "                id_citacion_pa = np.nan\n",
    "                usuario_ag = np.nan\n",
    "\n",
    "            # Diagnóstico “falso” coherente\n",
    "            codigo_dx = rng.choice([\"R10.4\", \"M54.5\", \"R51.9\", \"K76.9\", \"R93.7\"])\n",
    "            dx_texto = rng.choice([\n",
    "                \"Dolor abdominal\", \"Lumbalgia\", \"Cefalea\", \"Hallazgo imagenológico\",\n",
    "                \"Control postquirúrgico\", \"Traumatismo reciente\"\n",
    "            ])\n",
    "\n",
    "            # n° prestación derivada simulado\n",
    "            num_actual_prest = int(rng.integers(1_000_000, 1_999_999))\n",
    "\n",
    "            registros.append({\n",
    "                # --- columnas equivalentes a `campos` (sin corchetes SQL) ---\n",
    "                \"fecha\": f.normalize(),\n",
    "                \"episodio\": episodio_correlativo,\n",
    "                \"rut\": rut_fake(),\n",
    "                \"prestacion_der\": prest[i],\n",
    "                \"centro_medico\": rng.choice(centros),\n",
    "                \"zona\": rng.choice(zonas),\n",
    "                \"linea_negocio\": rng.choice(linea_negocio),\n",
    "                \"descr_especialidad\": espec[i],\n",
    "                \"B8\": rng.choice(B8_cat),\n",
    "                \"paciente\": int(rng.integers(100_000, 999_999)),\n",
    "                \"aseguradora\": \"PUB\",                     # código simulado\n",
    "                \"aseguradora_denom\": \"Seguro_Publico\",            # condición cumplida\n",
    "                \"precio_promedio\": float(np.round(precio, 0)),\n",
    "                \"derivacion\": 1,                          # derivación presente\n",
    "                \"captura\": int(captura_flag[i]),\n",
    "                \"episodio_der\": episodio_correlativo,     # coherente\n",
    "                \"cod_uo_medica\": int(rng.integers(1000, 9999)),\n",
    "                \"especialidad_der\": espec[i],\n",
    "                \"cod_uo_medica_cap\": int(rng.integers(1000, 9999)) if captura_flag[i] else np.nan,\n",
    "                \"especialidad_cap\": espec[i] if captura_flag[i] else np.nan,\n",
    "                \"linea_negocio_cap\": \"Imagenología\" if captura_flag[i] else np.nan,\n",
    "                \"prestacion_cap\": prest[i] if captura_flag[i] else np.nan,\n",
    "                \"fecha_cap\": fecha_cap,\n",
    "                \"episodio_cap\": episodio_cap,\n",
    "                \"centro_medico_cap\": centro_cap,\n",
    "                \"valor_liquidado\": valor_liquidado,\n",
    "                \"id_citacion_pa\": id_citacion_pa,\n",
    "                \"fecha_cita_pa\": fecha_cita_pa,\n",
    "                \"fecha_creacion_pa\": fecha_creacion_pa,\n",
    "                \"fecha_anulacion_pa\": fecha_anulacion_pa,\n",
    "                \"usuario_agendamiento_pa\": usuario_ag,\n",
    "                \"codigo_diagnostico\": codigo_dx,\n",
    "                \"diagnostico\": dx_texto,\n",
    "                \"numero_actual_prestacion_der\": num_actual_prest,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(registros)\n",
    "\n",
    "    # Asegurar tipos básicos coherentes\n",
    "    fecha_cols = [\"fecha\", \"fecha_cap\", \"fecha_cita_pa\", \"fecha_creacion_pa\", \"fecha_anulacion_pa\"]\n",
    "    for c in fecha_cols:\n",
    "        df[c] = pd.to_datetime(df[c])\n",
    "\n",
    "    # Reordenar columnas igual que tu lista `campos` (alias ya aplicados)\n",
    "    orden = [\n",
    "        \"fecha\",\"episodio\",\"rut\",\"prestacion_der\",\"centro_medico\",\"zona\",\n",
    "        \"linea_negocio\",\"descr_especialidad\",\"B8\",\"paciente\",\"aseguradora\",\n",
    "        \"aseguradora_denom\",\"precio_promedio\",\"derivacion\",\"captura\",\"episodio_der\",\n",
    "        \"cod_uo_medica\",\"especialidad_der\",\"cod_uo_medica_cap\",\n",
    "        \"especialidad_cap\",\"linea_negocio_cap\",\"prestacion_cap\",\n",
    "        \"fecha_cap\",\"episodio_cap\",\"centro_medico_cap\",\"valor_liquidado\",\"id_citacion_pa\",\n",
    "        \"fecha_cita_pa\",\"fecha_creacion_pa\",\"fecha_anulacion_pa\",\"usuario_agendamiento_pa\",\n",
    "        \"codigo_diagnostico\",\"diagnostico\",\"numero_actual_prestacion_der\"\n",
    "    ]\n",
    "    return df[orden].copy()\n",
    "\n",
    "# 3) “Extracción” simulada (reemplaza el bloque extraccion_paralela (placeholder))\n",
    "derivacion_captura = simular_derivacion_captura(inicio=finicio, fin=ffin, seed=2025, media_registros_dia=110)\n",
    "\n",
    "# 4) Limpieza similar a la tuya\n",
    "import gc; gc.collect()\n",
    "\n",
    "# 5) Transformación a tipo fecha (ya viene convertida, pero mantenemos tu línea)\n",
    "derivacion_captura['fecha'] = pd.to_datetime(derivacion_captura['fecha'])\n",
    "\n",
    "# Vistazo rápido\n",
    "cols_safe = [c for c in derivacion_captura.columns if c not in [\"rut\",\"paciente\",\"numero_actual_prestacion_der\"]]\n",
    "derivacion_captura[cols_safe].head(3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8c70d88-7000-4b56-b09d-e815f65d0044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83E\uDDF1 Construcción de la base histórica enriquecida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1901fbff-2595-46fe-8ee3-db028ca35876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Intentar leer los archivos Parquet desde las rutas obtenidas\n",
    "try:\n",
    "    # Leer los archivos Parquet en un solo DataFrame\n",
    "    final_df = spark.read.parquet(*ls_paths)\n",
    "    print(\"Datos cargados correctamente\")\n",
    "except Exception as e:\n",
    "    # Capturar y mostrar errores si ocurren\n",
    "    print(f\"Error al leer los archivos Parquet: {e}\")\n",
    "\n",
    "    \n",
    "# Aplicar filtros y transformación a PySpark DataFrame\n",
    "historico_derivacion_captura = final_df.filter(final_df.especialidad_cap.isin('Resonancia', 'Scanner'))\n",
    "\n",
    "#Filtra Isapre\n",
    "historico_derivacion_captura = historico_derivacion_captura.filter(historico_derivacion_captura.aseguradora_denom.isin('Seguro_Publico'))\n",
    "\n",
    "# Convertir a Pandas\n",
    "historico_derivacion_captura = historico_derivacion_captura.toPandas()\n",
    "\n",
    "# Eliminar el DataFrame original de PySpark\n",
    "del(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dc98b1bd-07cf-4096-8ecb-60c8c43bdba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Concatenar historico + 3 meses\n",
    "derivacion_captura = pd.concat([historico_derivacion_captura, derivacion_captura], axis=0)\n",
    "\n",
    "# Analisis de fecha\n",
    "# Formato fecha\n",
    "derivacion_captura['fecha'] = pd.to_datetime(derivacion_captura['fecha'])\n",
    "\n",
    "# Rango de fechas\n",
    "print(derivacion_captura.fecha.min())\n",
    "print(derivacion_captura.fecha.max())\n",
    "\n",
    "# Eliminar duplicados\n",
    "derivacion_captura = derivacion_captura.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f1cb9bb-445c-433c-847b-f871a4a8bf30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Transformación y Limpieza\n",
    "\n",
    "- **Variables categóricas**: transformadas mediante `StringIndexer`, permitiendo su uso en modelos como GBT.\n",
    "- **Variables numéricas**: ensambladas con `VectorAssembler` y estandarizadas con `StandardScaler` para mejorar el entrenamiento.\n",
    "- Se construye un `Pipeline` de transformación reutilizable, aplicado tanto al set de entrenamiento como de prueba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f528990e-b711-45cc-b49b-f92f669ebd19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83E\uDDCD Información del Paciente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc63fda7-bbe4-4153-a61f-4c5fd2fdef50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83D\uDD01 Comportamiento Histórico del Paciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8fc1a3ad-951c-4e32-a531-c3f430217cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transformaciones\n",
    "pacientes = calcular_edad(pacientes, 'fecha_nacimiento')\n",
    "\n",
    "# Mapear género\n",
    "mapa_genero = {\n",
    "    '1': 'Hombre',\n",
    "    '2': 'Mujer',\n",
    "    '3': 'No indica'\n",
    "}\n",
    "pacientes['Genero'] = pacientes['sexo'].map(mapa_genero).fillna('No indica')\n",
    "\n",
    "# Merge de pacientes con derivacion_captura\n",
    "derivacion_captura = derivacion_captura.merge(\n",
    "    pacientes[['paciente', 'rut', 'sexo', 'edad', 'Genero']],\n",
    "    on='paciente',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calcular la anticipación en días\n",
    "derivacion_captura['antelacion'] = (\n",
    "    pd.to_datetime(derivacion_captura['fecha_cap']) - \n",
    "    pd.to_datetime(derivacion_captura['fecha'])\n",
    ").dt.days\n",
    "\n",
    "# Extraer mes y año\n",
    "derivacion_captura['mes'] = pd.to_datetime(derivacion_captura['fecha']).dt.month\n",
    "derivacion_captura['año'] = pd.to_datetime(derivacion_captura['fecha']).dt.year\n",
    "\n",
    "# Crear columna 'año-mes'\n",
    "derivacion_captura['año-mes'] = (\n",
    "    derivacion_captura['año'].astype(str) + '-' + \n",
    "    derivacion_captura['mes'].apply(lambda x: f'{x:02}')\n",
    ")\n",
    "\n",
    "# Calcular revenue\n",
    "derivacion_captura['revenue'] = np.where(\n",
    "    (derivacion_captura['antelacion'] >= 10) & \n",
    "    (derivacion_captura['antelacion'] <= 30) & \n",
    "    (pd.to_datetime(derivacion_captura['fecha']) >= '2024-01-01'), \n",
    "    1, \n",
    "    0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9db4c01a-f11d-4ac9-bb7a-2d5fb2ec59be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Limpieza de datos\n",
    "df = derivacion_captura.drop_duplicates()\n",
    "df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "# Agrupación por episodio\n",
    "group_ep = df.groupby(['paciente', 'linea_negocio_cap', 'fecha', 'episodio']).agg({\n",
    "    'prestacion_der': 'nunique',\n",
    "    'derivacion': 'last',\n",
    "    'captura': 'sum',\n",
    "    'antelacion': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Ordenar por paciente y fecha (más reciente primero)\n",
    "group_ep = group_ep.sort_values(['paciente', 'fecha'], ascending=[True, False])\n",
    "group_ep['rank'] = group_ep.groupby(['paciente', 'linea_negocio_cap']).cumcount() + 1\n",
    "\n",
    "# Filtrar últimas 3 visitas (sin contar la más reciente)\n",
    "last_three = group_ep[(group_ep['rank'] > 1) & (group_ep['rank'] <= 4)].copy()\n",
    "\n",
    "# Calcular nuevas features que capturen comportamiento errático\n",
    "last_three['derivacion_no_nula'] = last_three['derivacion'].replace(0, np.nan)\n",
    "last_three['conversion_rate'] = last_three['captura'] / last_three['derivacion_no_nula']\n",
    "last_three['conversion_rate'] = last_three['conversion_rate'].fillna(0)\n",
    "last_three['captura_binaria'] = (last_three['captura'] > 0).astype(int)\n",
    "\n",
    "# Agregaciones con estadísticas robustas\n",
    "result2 = last_three.groupby(['paciente', 'linea_negocio_cap']).agg({\n",
    "    'derivacion': ['sum', lambda x: x.std(ddof=0)],\n",
    "    'captura': ['sum', lambda x: x.std(ddof=0)],\n",
    "    'prestacion_der': 'sum',\n",
    "    'episodio': 'nunique',\n",
    "    'antelacion': ['median', lambda x: x.std(ddof=0)],\n",
    "    'conversion_rate': ['median', lambda x: x.std(ddof=0)],\n",
    "    'captura_binaria': 'mean'\n",
    "})\n",
    "\n",
    "# Renombrar columnas\n",
    "result2.columns = [\n",
    "    'Q*Derivaciones', 'Derivacion_std',\n",
    "    'Q*Captura', 'Captura_std',\n",
    "    'Q*prestacion',\n",
    "    'Q*episodio',\n",
    "    'antelacion_prom', 'antelacion_std',\n",
    "    'conversion_rate_median', 'conversion_rate_std',\n",
    "    'frecuencia_captura'\n",
    "]\n",
    "\n",
    "result2 = result2.reset_index()\n",
    "\n",
    "# Imputar valores nulos con sentinel (-1) y crear flag de historial\n",
    "campos_historial = [\n",
    "    'Q*Derivaciones', 'Derivacion_std', 'Q*Captura', 'Captura_std',\n",
    "    'Q*prestacion', 'Q*episodio', 'antelacion_prom', 'antelacion_std',\n",
    "    'conversion_rate_median', 'conversion_rate_std', 'frecuencia_captura'\n",
    "]\n",
    "\n",
    "for col in campos_historial:\n",
    "    result2[col] = result2[col].fillna(-1)\n",
    "\n",
    "# Flag binario: ¿tiene historial suficiente?\n",
    "result2['has_historial'] = (result2['Q*Captura'] != -1).astype(int)\n",
    "\n",
    "# Ordenar\n",
    "result2 = result2.sort_values(by='Q*prestacion')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "565a325c-c6cc-4f6a-b68b-e066e9db86e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Modelo de datos\n",
    "base_modelo = (\n",
    "    derivacion_captura[['fecha', 'episodio', 'zona', 'linea_negocio_cap', 'descr_especialidad',\n",
    "                        'especialidad_cap', 'paciente', 'sexo', 'edad', 'Genero',\n",
    "                        'aseguradora_denom', 'año', 'año-mes', 'mes', 'derivacion',\n",
    "                        'captura', 'antelacion', 'revenue', 'precio_promedio']]\n",
    "    .merge(result2, on=['paciente', 'linea_negocio_cap'], how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbd3a42b-3cf1-481f-ae40-b00323d9831f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Selección de Variables\n",
    "\n",
    "Se definieron dos grupos de variables:\n",
    "\n",
    "- **Categóricas**:\n",
    "  - sexo, zona_t, descr_especialidad_t, aseguradora_t, especialidad_cap_t\n",
    "- **Numéricas**:\n",
    "  - edad, volumen histórico de derivaciones, capturas, prestaciones, episodios\n",
    "  - estadísticas de antelación, tasas de conversión, historial, revenue\n",
    "\n",
    "Estas variables reflejan comportamiento pasado del paciente y la naturaleza del servicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "177addb7-3bbe-4c81-966c-c26601679ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83D\uDD24 Codificación de Variables Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0e154bbb-826f-4222-b43c-d890d4cd9506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "df = base_modelo.copy() \n",
    "\n",
    "# Transformación de variables\n",
    "def t_encoder(var):\n",
    "    label_encoder = LabelEncoder()\n",
    "    return label_encoder.fit_transform(var)\n",
    "\n",
    "# Nuevas variables transformadas\n",
    "df['zona_t'] = t_encoder(df['zona'])\n",
    "df['descr_especialidad_t'] = t_encoder(df['descr_especialidad'])\n",
    "df['aseguradora_t'] = t_encoder(df['aseguradora_denom'])\n",
    "df['especialidad_cap_t'] = t_encoder(df['especialidad_cap'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1813f80e-52ce-41ca-893f-857d367b29f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83D\uDCCC Selección de Variables Relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3e2f2ac0-480b-4968-8189-7ba27792e7b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Seleccionar variables\n",
    "features = [\n",
    "    'paciente', 'fecha', 'sexo', 'edad', 'captura', 'antelacion', 'revenue',\n",
    "    'Q*Derivaciones',\n",
    "       'Derivacion_std', 'Q*Captura', 'Captura_std', 'Q*prestacion',\n",
    "       'Q*episodio', 'antelacion_prom', 'antelacion_std',\n",
    "       'conversion_rate_median', 'conversion_rate_std', 'frecuencia_captura',\n",
    "       'has_historial', 'zona_t', 'descr_especialidad_t', 'aseguradora_t',\n",
    "       'especialidad_cap_t'\n",
    "]\n",
    "\n",
    "df = df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dfc28fb-5339-4cbe-9e07-c75d98f7ce4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ⚠️ Manejo de Valores Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9601b2ab-0236-46a2-a775-9c04c1aac12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Variables demográficas\n",
    "df['sexo'] = df['sexo'].fillna(3)\n",
    "df['edad'] = df['edad'].fillna(df['edad'].median())\n",
    "\n",
    "# Variables derivadas de historial (usar sentinel)\n",
    "campos_historial = [\n",
    "    'Q*Derivaciones', 'Derivacion_std', 'Q*Captura', 'Captura_std',\n",
    "    'Q*prestacion', 'Q*episodio', 'antelacion_prom', 'antelacion_std',\n",
    "    'conversion_rate_median', 'conversion_rate_std', 'frecuencia_captura'\n",
    "]\n",
    "\n",
    "for col in campos_historial:\n",
    "    df[col] = df[col].fillna(-1)\n",
    "\n",
    "# Flag binario: ¿tiene historial?\n",
    "df['has_historial'] = (df['Q*Captura'] != -1).astype(int)\n",
    "\n",
    "# Para `antelacion`: si solo aplica cuando hay captura, rellena con -1\n",
    "df['antelacion'] = df['antelacion'].fillna(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26e108be-b3b5-4f90-ae70-a0817c635a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Entrenamiento del Modelo\n",
    "\n",
    "Se utilizó un modelo **Gradient-Boosted Trees (GBTClassifier)** por su capacidad de modelar relaciones no lineales y su buen rendimiento en problemas de clasificación binaria con variables mixtas.\n",
    "\n",
    "- Se aplicó `CrossValidator` con 3 folds y búsqueda en grilla (`maxDepth` y `maxIter`).\n",
    "- Se incorporó `weightCol` para **aumentar el peso de la clase positiva**, dada la desbalanceada entre pacientes capturados y no capturados.\n",
    "- Se entrenó y registró el mejor modelo usando **MLflow**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "81eaef6e-496c-483e-9522-926319ad1f1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Modelo para revenue\n",
    "df_model = df\n",
    "\n",
    "#Eliminar duplicados\n",
    "df_model = df_model.drop_duplicates()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame if necessary\n",
    "if isinstance(df_model, pd.DataFrame):\n",
    "    df_model = spark.createDataFrame(df_model)\n",
    "\n",
    "# Ensure the correct data type for 'fecha'\n",
    "df_model = df_model.withColumn(\"fecha\", F.to_date(df_model[\"fecha\"], \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed228aca-613e-42cb-955f-2599d4773633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ML: GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8c29380d-7d9c-4cf6-9647-d3e071b95697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.spark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear sesión Spark\n",
    "spark = SparkSession.builder.appName(\"GBT_Modelo\").getOrCreate()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Fechas y partición de datos\n",
    "# ---------------------------------------------\n",
    "fecha_corte_f = '2025-04-01'\n",
    "fecha_corte_i = '2022-01-01'\n",
    "\n",
    "train = df_model.filter((df_model['fecha'] >= fecha_corte_i) & (df_model['fecha'] < fecha_corte_f))\n",
    "test = df_model.filter(df_model['fecha'] >= fecha_corte_f)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Definición de variables\n",
    "# ---------------------------------------------\n",
    "cat_features = [\n",
    "    'sexo', 'zona_t', 'descr_especialidad_t', 'aseguradora_t', 'especialidad_cap_t'\n",
    "]\n",
    "\n",
    "num_features = [\n",
    "    'edad', 'Q*Derivaciones', 'Derivacion_std', 'Q*Captura', 'Captura_std',\n",
    "    'Q*prestacion', 'Q*episodio', 'antelacion_prom', 'antelacion_std',\n",
    "    'conversion_rate_median', 'conversion_rate_std', 'frecuencia_captura',\n",
    "    'has_historial', 'revenue'\n",
    "]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Preprocesamiento\n",
    "# ---------------------------------------------\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid='keep') for col in cat_features]\n",
    "assembler_num = VectorAssembler(inputCols=num_features, outputCol=\"num_features_vec\")\n",
    "scaler = StandardScaler(inputCol=\"num_features_vec\", outputCol=\"scaled_num_features\")\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[f\"{col}_index\" for col in cat_features] + [\"scaled_num_features\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [assembler_num, scaler, final_assembler])\n",
    "pipeline_model = pipeline.fit(train)\n",
    "\n",
    "train_data = pipeline_model.transform(train).cache()\n",
    "test_data = pipeline_model.transform(test).cache()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Agregar pesos a clases (weightCol)\n",
    "# ---------------------------------------------\n",
    "train_data = train_data.withColumn(\"weight\", when(col(\"captura\") == 1, 3.0).otherwise(1.0))\n",
    "\n",
    "train_count = train_data.count()\n",
    "test_count = test_data.count()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. MLflow + Entrenamiento\n",
    "# ---------------------------------------------\n",
    "with mlflow.start_run(nested=True):\n",
    "\n",
    "    mlflow.log_params({\n",
    "        \"fecha_corte_inicio\": fecha_corte_i,\n",
    "        \"fecha_corte_fin\": fecha_corte_f,\n",
    "        \"num_train_samples\": train_count,\n",
    "        \"num_test_samples\": test_count,\n",
    "        \"scaler\": \"StandardScaler\",\n",
    "        \"model_type\": \"GBTClassifier\",\n",
    "        \"weightCol\": \"yes\",\n",
    "        \"positive_class_weight\": 3.0\n",
    "    })\n",
    "\n",
    "    gbt = GBTClassifier(\n",
    "        labelCol=\"captura\",\n",
    "        featuresCol=\"features\",\n",
    "        seed=42,\n",
    "        maxIter=50,\n",
    "        maxBins=100,\n",
    "        weightCol=\"weight\"\n",
    "    )\n",
    "\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(gbt.maxDepth, [4, 6, 8]) \\\n",
    "        .addGrid(gbt.maxIter, [30, 50]) \\\n",
    "        .build()\n",
    "\n",
    "    evaluator_pr = BinaryClassificationEvaluator(labelCol=\"captura\", metricName=\"areaUnderPR\")\n",
    "\n",
    "    crossval = CrossValidator(\n",
    "        estimator=gbt,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=evaluator_pr,\n",
    "        numFolds=3\n",
    "    )\n",
    "\n",
    "    cvModel = crossval.fit(train_data)\n",
    "    predictions = cvModel.transform(test_data)\n",
    "\n",
    "    pr_auc = evaluator_pr.evaluate(predictions)\n",
    "    roc_auc = BinaryClassificationEvaluator(labelCol=\"captura\", metricName=\"areaUnderROC\").evaluate(predictions)\n",
    "\n",
    "    mlflow.log_metric(\"areaUnderPR\", pr_auc)\n",
    "    mlflow.log_metric(\"areaUnderROC\", roc_auc)\n",
    "\n",
    "    print(f\"Área bajo curva PR: {pr_auc}\")\n",
    "    print(f\"Área bajo curva ROC: {roc_auc}\")\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Convertir columna probability a vector accesible\n",
    "predictions = predictions.withColumn(\"prob_array\", vector_to_array(\"probability\"))\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Buscar mejor umbral por F1\n",
    "# ---------------------------------------------\n",
    "def buscar_mejor_umbral(predictions, label_col=\"captura\", prob_col=\"prob_array\", metric=\"f1\", plot=True):\n",
    "    thresholds = [x / 100 for x in range(10, 90, 5)]\n",
    "    scores = []\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction_temp\", metricName=metric)\n",
    "\n",
    "    for t in thresholds:\n",
    "        temp_pred = predictions.withColumn(\n",
    "            \"prediction_temp\", when(col(prob_col)[1] >= t, 1.0).otherwise(0.0)\n",
    "        )\n",
    "        score = evaluator.evaluate(temp_pred)\n",
    "        scores.append(score)\n",
    "\n",
    "    best_idx = scores.index(max(scores))\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_score = scores[best_idx]\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(thresholds, scores, marker=\"o\")\n",
    "        plt.title(f\"{metric.upper()} vs Threshold\")\n",
    "        plt.xlabel(\"Threshold\")\n",
    "        plt.ylabel(metric.upper())\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return {\n",
    "        \"mejor_umbral\": best_threshold,\n",
    "        f\"mejor_{metric}\": best_score,\n",
    "        \"todos_los_resultados\": list(zip(thresholds, scores))\n",
    "    }\n",
    "\n",
    "# Buscar umbral óptimo por F1\n",
    "resultado = buscar_mejor_umbral(predictions, metric=\"f1\", plot=True)\n",
    "mejor_umbral = resultado[\"mejor_umbral\"]\n",
    "\n",
    "mlflow.log_metric(\"mejor_umbral_f1\", mejor_umbral)\n",
    "mlflow.log_metric(\"mejor_f1\", resultado[\"mejor_f1\"])\n",
    "\n",
    "print(f\"✅ Mejor umbral F1: {mejor_umbral:.2f} - F1: {resultado['mejor_f1']:.4f}\")\n",
    "\n",
    "# Aplicar predicción con umbral óptimo\n",
    "predictions = predictions.withColumn(\n",
    "    \"prediction_custom\",\n",
    "    when(col(\"prob_array\")[1] >= mejor_umbral, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 7. Evaluar con Recall, Precision y F1\n",
    "# ---------------------------------------------\n",
    "print(\"\uD83D\uDCCA Métricas de evaluación con umbral ajustado:\")\n",
    "\n",
    "for metric in [\"f1\", \"precisionByLabel\", \"recallByLabel\"]:\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"captura\",\n",
    "        predictionCol=\"prediction_custom\",\n",
    "        metricName=metric\n",
    "    )\n",
    "    value = evaluator.evaluate(predictions)\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "    mlflow.log_metric(f\"{metric}_custom_threshold\", value)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 8. Matriz de confusión\n",
    "# ---------------------------------------------\n",
    "print(f\"\uD83D\uDCCA Matriz de confusión con umbral ajustado = {mejor_umbral}\")\n",
    "predictions.groupBy(\"captura\", \"prediction_custom\").count().orderBy(\"captura\", \"prediction_custom\").show()\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 9. Guardar Modelo\n",
    "# -----------------------------------------------\n",
    "import time, json\n",
    "from pyspark.ml import PipelineModel\n",
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "# A) Ensamblar un modelo final end-to-end: preproceso + bestModel\n",
    "final_model = PipelineModel(stages=pipeline_model.stages + [cvModel.bestModel])\n",
    "\n",
    "# B) Paths de guardado en DBFS (uno fijo y otro versionado por timestamp)\n",
    "BASE_PATH = \"dbfs:/Modelos/model_gbt_fonasa\"\n",
    "VERSION_PATH = f\"{BASE_PATH}/v={int(time.time())}\"\n",
    "\n",
    " # Guardar modelo\n",
    "best_mlp_model = cvModel.bestModel\n",
    "mlflow.spark.log_model(best_mlp_model, \"dbfs:/Modelos/model_gbt_fonasa/best_model\")\n",
    "best_mlp_model.write().overwrite().save(\"dbfs:/Modelos/model_gbt_fonas\")\n",
    "\n",
    "# Guardar en DBFS\n",
    "final_model.write().overwrite().save(BASE_PATH)\n",
    "final_model.write().overwrite().save(VERSION_PATH)\n",
    "\n",
    "# C) Asegurar una run activa para loguear a MLflow (por si el 'with ...' anterior ya terminó)\n",
    "active_run = mlflow.active_run()\n",
    "if active_run is None:\n",
    "    mlflow.start_run(nested=True)\n",
    "\n",
    "# Loguear metadatos útiles\n",
    "mlflow.log_param(\"decision_threshold\", float(mejor_umbral))\n",
    "mlflow.log_dict(\n",
    "    {\n",
    "        \"fecha_corte_inicio\": fecha_corte_i,\n",
    "        \"fecha_corte_fin\": fecha_corte_f,\n",
    "        \"cat_features\": cat_features,\n",
    "        \"num_features\": num_features,\n",
    "        \"weightCol\": \"weight\",\n",
    "        \"positive_class_weight\": 3.0,\n",
    "        \"dbfs_save_path_latest\": BASE_PATH,\n",
    "        \"dbfs_save_path_versioned\": VERSION_PATH\n",
    "    },\n",
    "    \"model_metadata.json\"\n",
    ")\n",
    "\n",
    "# (Opcional) imprimir confirmación\n",
    "print(\"✅ Modelo guardado en:\")\n",
    "print(f\"   - Última versión: {BASE_PATH}\")\n",
    "print(f\"   - Versionado:     {VERSION_PATH}\")\n",
    "print(\"✅ Modelo logueado en MLflow (artefacto 'model').\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52dca2bc-01d3-4486-b29f-8ac1936b5655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Evaluación del Modelo\n",
    "\n",
    "El modelo fue evaluado con las siguientes métricas:\n",
    "\n",
    "- **Área bajo curva PR (Precision-Recall)**: 0.712\n",
    "- **Área bajo curva ROC**: 0.769\n",
    "- Se ajustó el umbral de decisión usando F1-score, resultando en:\n",
    "  - **Umbral óptimo**: 0.80\n",
    "  - **F1-score**: 0.761\n",
    "  - **Precisión clase positiva**: 0.78\n",
    "  - **Recall clase positiva**: 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7c49a7df-88a4-4a69-9677-961e2593f1ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# 1. Convertir probabilidad a array\n",
    "predictions = predictions.withColumn(\"prob_array\", vector_to_array(\"probability\"))\n",
    "\n",
    "# 2. Definir umbral óptimo (puedes cambiarlo si encontraste otro mejor)\n",
    "umbral_optimo = 0.80\n",
    "\n",
    "# 3. Crear predicción customizada (opcional, para métricas)\n",
    "predictions = predictions.withColumn(\n",
    "    \"prediction_custom\",\n",
    "    when(col(\"prob_array\")[1] >= umbral_optimo, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# 4. Normalizar la probabilidad de captura\n",
    "predictions = predictions.withColumn(\n",
    "    \"prob_normalizada\",\n",
    "    (col(\"prob_array\")[1] / umbral_optimo).cast(\"double\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "383a9df5-866d-426e-9203-7a02393cf695",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Aplicación y Lógica de Negocio\n",
    "\n",
    "Las predicciones del modelo se utilizan como insumo para diseñar acciones diferenciadas según el nivel de probabilidad estimada.\n",
    "\n",
    "Para los casos con **baja probabilidad de conversión**, se aplican **incentivos más atractivos** con el fin de aumentar la motivación.\n",
    "\n",
    "Para los casos con **probabilidad intermedia**, se ofrece un **estímulo moderado**, buscando equilibrar costo y beneficio.\n",
    "\n",
    "En los casos con **alta probabilidad**, se **evita otorgar beneficios** innecesarios, ya que es probable que el cliente concrete la acción de todas formas.\n",
    "\n",
    "De esta manera, se logra **maximizar la efectividad de las acciones comerciales**, concentrando los recursos donde son más necesarios y reduciendo el costo de incentivos superfluos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17e87562-f410-4675-9b90-b3ed826e39ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Conclusiones\n",
    "\n",
    "El modelo entrega una predicción robusta de la probabilidad de que un paciente se atienda, lo cual permite implementar estrategias de pricing inteligente con descuentos personalizados.\n",
    "\n",
    "Su rendimiento permite impactar directamente la **rentabilidad de la operación**, enfocando recursos comerciales en los pacientes de menor probabilidad de captura sin sobreincentivar al resto."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Prediccion_Captura_Examenes_Medicos_GBTClassifier_PySpark_PORTFOLIO",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}